{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- seq2seq : 언어를 다른언어로 해석해주는 뉴럴기계번역 모델.\n",
    "\n",
    "-> 시퀀스를 입력받아 또 다른 시퀀스를 출력. 문장을 다른문장으로 번역해주는 모델이다.\n",
    "\n",
    "-> 각자 다른역할을 하는 두개의 RNN을 이어붙인 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습하기 위해서는 병렬 말뭉치라는 원문과 번역문이 쌍을 이루는 형태의 많은 텍스트데이터가 필요하다. \n",
    "- 예제는 매우 간소화 한 예제, 영어문자열 \"hello\"를 스페인어 문자열 \"hola\"로 번역하는 미니 seq2seq 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq 동작 설명\\\n",
    "\n",
    "1. 원문을 이해하고(인코더) 번역문을 작성하는(디코더) 두가지 동작으로 (두 RNN으로)구성.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 인코더\n",
    "\n",
    "개념\n",
    "- 원문속 모든 단어를 입력받아 문장의 뜻을 내포하는 하나의 고정크기텐서를 만듬\n",
    "- 압축된 텐서는 원문의 뜻과 내용을 압축하고 있다 하여 \"문맥벡터\"라고한다.\n",
    "- 오토인코더는 정보를 추려서 압축, rnn은 동적인 시계열 데이터를 정적인 시계열데이터로 압축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 영단어 hello를 hola로 바꾸는 것이므로 단어단위의 워드임베딩이 아닌 글자단위의 캐릭터임베딩을 사용\n",
    "vocab_size=256 # 데이터속에 총ㄷ 몇종류의 토큰이 있는지 정의. 영문만 다루므로 아스키코드로 대신, 아스키코드로는 총 256개의 글자표현가능\n",
    "x_=list(map(ord,\"hello\")) #아스키코드로 변환\n",
    "y_=list(map(ord,\"hola\"))\n",
    "x=torch.LongTensor(x_) # 파이토치 텐서로 변환\n",
    "y=torch.LongTensor(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.n_layers = 1\n",
    "        self.hidden_size = hidden_size #임베딩된 토큰의 차원값\n",
    "        #원래는 인코더와 디코더의 체계가 다른경우를 대비해 따로 만들어야하는데 예제에서는 모두 아스키코드로 나타내므로 임베딩은 하나만 만든다\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.encoder = nn.GRU(hidden_size, hidden_size) #인코더와 디코더를 gru로 대체\n",
    "        self.decoder = nn.GRU(hidden_size, hidden_size)\n",
    "        self.project = nn.Linear(hidden_size, vocab_size) #디코더가 번역문의 다음토큰을 예상해내는 작은 신경망\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # 인코더에 들어갈 입력\n",
    "        initial_state = self._init_state() #첫번째 은닉벡터를 정의.\n",
    "        embedding = self.embedding(inputs).unsqueeze(1)#원문의 모든 문자를 임베딩\n",
    "        # embedding = [seq_len, batch_size, embedding_size]\n",
    "        \n",
    "        # 인코더 (Encoder)\n",
    "        # 문맥벡터인 encoder_state를 만들고 이를 디코더의 첫번째 은닉벡터로 지정.\n",
    "        encoder_output, encoder_state = self.encoder(embedding, initial_state)\n",
    "        # encoder_output = [seq_len, batch_size, hidden_size]\n",
    "        # encoder_state  = [n_layers, seq_len, hidden_size]\n",
    "\n",
    "        # 디코더에 들어갈 입력\n",
    "        decoder_state = encoder_state\n",
    "        decoder_input = torch.LongTensor([0]) #아스키값으로 null문자를 뜻한 0을 넣어줌으로써 디코더에 문장의 시작을 알림\n",
    "        \n",
    "        # 디코더 (Decoder)\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(targets.size()[0]):\n",
    "            decoder_input = self.embedding(decoder_input).unsqueeze(1)\n",
    "            decoder_output, decoder_state = self.decoder(decoder_input, decoder_state)\n",
    "            projection = self.project(decoder_output) #다음 예상글자를 예측.\n",
    "            outputs.append(projection)\n",
    "            \n",
    "            #티처 포싱(Teacher Forcing) 사용\n",
    "            decoder_input = torch.LongTensor([targets[i]])\n",
    "\n",
    "        outputs = torch.stack(outputs).squeeze()\n",
    "        return outputs\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_size).zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
