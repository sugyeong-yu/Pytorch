# 딥러닝을 해킹하는 적대적 공격
## 8.1 적대적 공격의미
**머신러닝 기반 시스템의 성능을 의도적으로 떨어뜨려 보안문제를 일으키는 것을 적대적 공격, Adversarial attack이라 한다.**

- 입력데이터는 신경망 모델을 타고 흐르면서 변환이 계속되는데 각각의 변환은 입력의 특정구조에 매우 예민하게 반응한다. 이런 예민한 부분을 공략함으로써 모델을 헷갈리게 할 수 있다.
- torchvision의 학습된 모델을 가져와 ResNet과 DenseNet과 같은 복잡한 모델을 무력화하는 것이 얼마나 쉬운지 실습해본다.
## 8.2 적대적 공격의 종류
- 사람의 눈에는 같아보이지만 머신러닝 모델을 헷갈리게하는 적대적 예제를 생성하는 것이 핵심이다.
- 모델의 오차를 줄이기 보다는 극대화 하여 잡음을 최적화 한다.
## 8.3 FGSM공격
