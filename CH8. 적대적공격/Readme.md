# 딥러닝을 해킹하는 적대적 공격
## 8.1 적대적 공격의미
**머신러닝 기반 시스템의 성능을 의도적으로 떨어뜨려 보안문제를 일으키는 것을 적대적 공격, Adversarial attack이라 한다.**

- 입력데이터는 신경망 모델을 타고 흐르면서 변환이 계속되는데 각각의 변환은 입력의 특정구조에 매우 예민하게 반응한다. 이런 예민한 부분을 공략함으로써 모델을 헷갈리게 할 수 있다.
- torchvision의 학습된 모델을 가져와 ResNet과 DenseNet과 같은 복잡한 모델을 무력화하는 것이 얼마나 쉬운지 실습해본다.
## 8.2 적대적 공격의 종류
- 사람의 눈에는 같아보이지만 머신러닝 모델을 헷갈리게하는 적대적 예제를 생성하는 것이 핵심이다.
- 모델의 오차를 줄이기 보다는 극대화 하여 잡음을 최적화 한다.
- 적대적 예제에서 잡음의 생성방법은 분류기준이 무엇이냐에 따라 나뉜다.
- 모델의 분류기준
1. 기울기와 같은 모델정보가 필요한지에 따라, 모델정보를 토대로 잡음을 생상하는 화이트박스방법과 모델정보없이 생성하는 블랙박스로 나뉜다.
2. 원하는 정답으로 유도할 수 있도록 표적 아니라면 비표적 으로 분류한다.
3. 잡음을 생성하기 위해 반복된학습이 필요하면 반복, 아니라면 원샷 으로 나뉜다.
4. 한 잡음이 특정 입력에만 적용되는지, 혹은 모든이미지에 적용되는 범용적잡음인지 나뉜다.
- 가장 강력한 공격방법은 모델정보가 필요없고 원하는 정보로 유도할 수 있고 복잡한 학습이 필요하지 않으며 여러모델에 동시에 적용할수있는 방법이다. 그러나 각 특징에는 기회비용이 존재한다.
## 8.3 FGSM공격
이번 예제에서는 FGSM(Fast Gradient sign method)라는 방법으로 적대적 예제를 생성해, 미리 학습된 딥러닝 모델을 공격해본다.
-FGSM은 가장 간단하면서 효과적인 공격방식이다.
-반복된 학습없이 잡음을 생성하는 원샷공격으로 입력이미지에 대한 기울기의 정보를 추출하여 잡음을 생성한다.
- 최적화를 반복하여 더 정교한 잡음을 만들 수 있다.
- FGSM은 공격목표를 정할 수 있는 non-targeted방식이자, 대상 모델의 정보가 필요한 화이트박스 방식이다.
- fgsm_attack.ipynb 참고
