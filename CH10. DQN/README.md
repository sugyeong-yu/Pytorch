# DQN
강화학습은 지도학습이나 비지도학습과는 다른 학습방식이다. 강화학습이 어떻게 이루어지는지 간단한 게임을 하는 마스터 모델을 만들며 알아본다.
## 10.1 강화학습과 DQN 기초
- 강화학습은 주어진 환경과 상호작용하여 좋은 점수를 얻는 방향으로 성장하는 머신러닝 분야이다.
- 앞에서 배운 학습법은 주입식 학습법이라면 강화학습은 자기주도적 학습법이라고 할 수 있다.
- 강화학습모델도 주어진환경에서 시행착오를 겪으며 좋은 피드백을 받는쪽으로 최적화하며 성정한다.
- 따라서 강화학습은 크게  state, agent, action, reward 4가지요소로 나눌 수 있다.
1. agent : 인공지능 플레이어다.
2. state : agent가 솔루션을 찾기위한 무대
3. action : agent 환경안에서 시행하는 상호작용
4. reward : agent의 action에 따른 점수 혹은 결과\
![image](https://user-images.githubusercontent.com/70633080/104834088-97f0ee80-58e0-11eb-8c4e-939e02a394b5.png)

## 10.2 카트폴 게임 마스터하기
대부분의 게임은 점수나 목표가 있다. 점수가 오르거나 목표에 도달하게되면 일종의 보상을 받고 원치않는 행동을 할 때는 마이너스 보상을 받는 경우가 있다. 이에 게임 중에서도 가장 간단한 카트폴이라는 환경을 구축하여 강화학습을 배워본다.
![image](https://user-images.githubusercontent.com/70633080/104834212-53198780-58e1-11eb-8755-d027bd0e2943.png)
- 카트폴 게임 : 막대기를 세우고 오래 버틸수록 점수가 올라간다. 막대기가 오른쪽으로 기울었을때 어떤 동작이 가장 큰 보상을 준다고 예측할수 있을까? 오른쪽 버튼을 눌러 검은색 상자를 오른쪽으로 움직이는 것이 왼쪽을 눌러 검은상자를 왼쪽으로 옮기는것 보다 보상으 클 것을 예측할 수 있다.
- DQN은 두가지 주요특징이 있다.
1. 기억하기(memorize)
2. 다시보기(replay)
- 이에 순서대로 개념과 구현법을 알아본다
- cartpole_dqn.ipynb 참고
### 10.2.1
### 10.2.2 DQN 에이전트
- DQN Class를 만든다.
- 아래그림은 DQN 에이전트의 인공신경망모습이다.\
![image](https://user-images.githubusercontent.com/70633080/104834876-fa002280-58e5-11eb-94db-55534d368845.png)
- 카트위치, 속도, 막대기 각도, 속도 총 4가지입력을 받아 왼쪽으로 갈때의 가치와 오른쪽으로 갈때의 가치를 출력한다. 
### 10.2.3 이전경험 기억하기
- DQN이 나오기전 신경망을 강화학습에 사용하며 겪는 몇가지 문제가 있었다.
1. 딥러닝 모델들은 보통 학습데이터샘플이 각각 독립적이라 가정한다. 그러나 강화학습에서는 연속된 상태가 강한 상관관계가 있어서 학습이 어렵다는 것이다.
2. 신경망이 새로운 경험을 전경험에 겹쳐쓰면서 쉽게 잊어버리게 된다.
- 이를 해결한 것이 기억하기 기능이다.
- 이전경험들을 배열에 담아 계속 재학습을 시키며 신경망이 잊지않게 한다는 아이디어이다.
- 기억한 경험들은 학습할때 무작위로 뽑아 경험간의 상관관계를 줄인다. 
