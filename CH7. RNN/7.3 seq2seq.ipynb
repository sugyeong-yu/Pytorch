{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- seq2seq : 언어를 다른언어로 해석해주는 뉴럴기계번역 모델.\n",
    "\n",
    "    - 시퀀스를 입력받아 또 다른 시퀀스를 출력. 문장을 다른문장으로 번역해주는 모델이다.\n",
    "\n",
    "    - 각자 다른역할을 하는 두개의 RNN을 이어붙인 모델이다.\n",
    "- 학습하기 위해서는 병렬 말뭉치라는 원문과 번역문이 쌍을 이루는 형태의 많은 텍스트데이터가 필요하다. \n",
    "- 예제는 매우 간소화 한 예제, 영어문자열 \"hello\"를 스페인어 문자열 \"hola\"로 번역하는 미니 seq2seq 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq 동작 설명\n",
    "\n",
    "1. 원문을 이해하고(인코더) 번역문을 작성하는(디코더) 두가지 동작으로 (두 RNN으로)구성.\n",
    "2. 인코더\n",
    "    - 원문속 모든 단어를 입력받아 문장의 뜻을 내포하는 하나의 고정크기텐서를 만듬\n",
    "    - 압축된 텐서는 원문의 뜻과 내용을 압축하고 있다 하여 \"문맥벡터\"라고한다.\n",
    "    - 오토인코더는 정보를 추려서 압축, rnn은 동적인 시계열 데이터를 정적인 시계열데이터로 압축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 간단한 영단어 hello를 hola로 바꾸는 것이므로 단어단위의 워드임베딩이 아닌 글자단위의 캐릭터임베딩을 사용\n",
    "vocab_size=256 # 데이터속에 총ㄷ 몇종류의 토큰이 있는지 정의. 영문만 다루므로 아스키코드로 대신, 아스키코드로는 총 256개의 글자표현가능\n",
    "x_=list(map(ord,\"hello\")) #아스키코드로 변환\n",
    "y_=list(map(ord,\"hola\"))\n",
    "x=torch.LongTensor(x_) # 파이토치 텐서로 변환\n",
    "y=torch.LongTensor(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<map object at 0x000001B577885208>\n",
      "[104, 101, 108, 108, 111]\n"
     ]
    }
   ],
   "source": [
    "print(map(ord,\"hello\"))\n",
    "print(list(map(ord,\"hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.n_layers = 1\n",
    "        self.hidden_size = hidden_size #임베딩된 토큰의 차원값\n",
    "        #원래는 인코더와 디코더의 체계가 다른경우를 대비해 따로 만들어야하는데 예제에서는 모두 아스키코드로 나타내므로 임베딩은 하나만 만든다\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.encoder = nn.GRU(hidden_size, hidden_size) #인코더와 디코더를 gru로 대체\n",
    "        self.decoder = nn.GRU(hidden_size, hidden_size)\n",
    "        self.project = nn.Linear(hidden_size, vocab_size) #디코더가 번역문의 다음토큰을 예상해내는 작은 신경망\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # 인코더에 들어갈 입력\n",
    "        initial_state = self._init_state() #첫번째 은닉벡터를 정의.\n",
    "        embedding = self.embedding(inputs).unsqueeze(1)#원문의 모든 문자를 임베딩\n",
    "        # embedding = [seq_len, batch_size, embedding_size]\n",
    "        \n",
    "        # 인코더 (Encoder)\n",
    "        # 문맥벡터인 encoder_state를 만들고 이를 디코더의 첫번째 은닉벡터로 지정.\n",
    "        encoder_output, encoder_state = self.encoder(embedding, initial_state)\n",
    "        # encoder_output = [seq_len, batch_size, hidden_size]\n",
    "        # encoder_state  = [n_layers, seq_len, hidden_size]\n",
    "\n",
    "        # 디코더에 들어갈 입력\n",
    "        decoder_state = encoder_state # 문맥벡터\n",
    "        decoder_input = torch.LongTensor([0]) #아스키값으로 null문자를 뜻한 0을 넣어줌으로써 디코더에 문장의 시작을 알림\n",
    "        \n",
    "        # 디코더 (Decoder)\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(targets.size()[0]):\n",
    "            decoder_input = self.embedding(decoder_input).unsqueeze(1)\n",
    "            decoder_output, decoder_state = self.decoder(decoder_input, decoder_state)\n",
    "            projection = self.project(decoder_output) #다음 예상글자를 예측.\n",
    "            outputs.append(projection)\n",
    "            \n",
    "            #티처 포싱(Teacher Forcing) 사용 (디코더의 출력이아닌 정답을 다음 은닉벡터에 넣어줌)\n",
    "            decoder_input = torch.LongTensor([targets[i]])\n",
    "\n",
    "        outputs = torch.stack(outputs).squeeze()\n",
    "        return outputs\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_size).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'th' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-51f84dacf63f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSeq2Seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'th' is not defined"
     ]
    }
   ],
   "source": [
    "seq2seq=Seq2Seq(vocab_size,16)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=th.optim.Adam(seq2seq.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[]\n",
    "for i in range(1000):\n",
    "    prediction=seq2seq(x,y)\n",
    "    loss=criterion(prediction,y)\n",
    "    optimizer.zero.grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_val=loss.data\n",
    "    log.append(loss_val)\n",
    "    if i%100 ==0:\n",
    "        print(\"\\n 반복:%d 오차 %s \" % (i,loss_val.item()))\n",
    "        _,top1=prediction.data.topk(1,1)\n",
    "        print([chr(c) for c in top1.squeeze().numpy().tolist()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
